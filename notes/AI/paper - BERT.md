---

type : knowledges
detail : NLP
content_type : paper
annotation-target: BERT.pdf

---

[[AI]]
**tags** : #ðŸ–¥ï¸note/AI/Paper 


>%%
>```annotation-json
>{"created":"2022-03-09T07:55:29.323Z","text":"Abstract","updated":"2022-03-09T07:55:29.323Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":208,"end":216},{"type":"TextQuoteSelector","exact":"Abstract","prefix":"ang,kentonl,kristout}@google.com","suffix":"We introduce a new language repr"}]}]}
>```
>%%
>*%%PREFIX%%ang,kentonl,kristout}@google.com%%HIGHLIGHT%% ==Abstract== %%POSTFIX%%We introduce a new language repr*
>%%LINK%%[[#^2u26epoo7bn|show annotation]]
>%%COMMENT%%
>Abstract
>%%TAGS%%
>
^2u26epoo7bn


>%%
>```annotation-json
>{"created":"2022-03-09T07:58:15.188Z","updated":"2022-03-09T07:58:15.188Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":444,"end":596},{"type":"TextQuoteSelector","exact":"BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on bothleft and right context in all layers.","prefix":" 2018a; Rad-ford et al., 2018), ","suffix":" As a re-sult, the pre-trained B"}]}]}
>```
>%%
>*%%PREFIX%%2018a; Rad-ford et al., 2018),%%HIGHLIGHT%% ==BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on bothleft and right context in all layers.== %%POSTFIX%%As a re-sult, the pre-trained B*
>%%LINK%%[[#^xk9dr83f3b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xk9dr83f3b


>%%
>```annotation-json
>{"created":"2022-03-09T08:00:23.191Z","updated":"2022-03-09T08:00:23.191Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":627,"end":726},{"type":"TextQuoteSelector","exact":"BERT model can be fine-tuned with just one additional output layerto create state-of-the-art models","prefix":". As a re-sult, the pre-trained ","suffix":" for a widerange of tasks, such "}]}]}
>```
>%%
>*%%PREFIX%%. As a re-sult, the pre-trained%%HIGHLIGHT%% ==BERT model can be fine-tuned with just one additional output layerto create state-of-the-art models== %%POSTFIX%%for a widerange of tasks, such*
>%%LINK%%[[#^7yoqko2eja7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7yoqko2eja7


>%%
>```annotation-json
>{"created":"2022-03-09T08:07:56.419Z","text":"1 Introduction","updated":"2022-03-09T08:07:56.419Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":1276,"end":1290},{"type":"TextQuoteSelector","exact":"1 Introduction","prefix":"5.1 point absolute improvement).","suffix":"Language model pre-training has "}]}]}
>```
>%%
>*%%PREFIX%%5.1 point absolute improvement).%%HIGHLIGHT%% ==1 Introduction== %%POSTFIX%%Language model pre-training has*
>%%LINK%%[[#^gom72aonwvl|show annotation]]
>%%COMMENT%%
>1 Introduction
>%%TAGS%%
>
^gom72aonwvl


>%%
>```annotation-json
>{"created":"2022-03-09T08:17:31.321Z","updated":"2022-03-09T08:17:31.321Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":1952,"end":2089},{"type":"TextQuoteSelector","exact":"There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning.","prefix":", 2003; Rajpurkar et al., 2016).","suffix":" Thefeature-based approach, such"}]}]}
>```
>%%
>*%%PREFIX%%, 2003; Rajpurkar et al., 2016).%%HIGHLIGHT%% ==There are two existing strategies for apply-ing pre-trained language representations to down-stream tasks: feature-based and fine-tuning.== %%POSTFIX%%Thefeature-based approach, such*
>%%LINK%%[[#^2eypu4a1set|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2eypu4a1set


>%%
>```annotation-json
>{"created":"2022-03-09T09:16:10.937Z","updated":"2022-03-09T09:16:10.937Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":2093,"end":2129},{"type":"TextQuoteSelector","exact":"feature-based approach, such as ELMo","prefix":"ature-based and fine-tuning. The","suffix":" (Peterset al., 2018a), uses tas"}]}]}
>```
>%%
>*%%PREFIX%%ature-based and fine-tuning. The%%HIGHLIGHT%% ==feature-based approach, such as ELMo== %%POSTFIX%%(Peterset al., 2018a), uses tas*
>%%LINK%%[[#^qxxmegn22sj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qxxmegn22sj


>%%
>```annotation-json
>{"created":"2022-03-09T09:17:59.871Z","updated":"2022-03-09T09:17:59.871Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":2259,"end":2338},{"type":"TextQuoteSelector","exact":"fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT)","prefix":"ns as addi-tional features. The ","suffix":" (Radford et al., 2018), introdu"}]}]}
>```
>%%
>*%%PREFIX%%ns as addi-tional features. The%%HIGHLIGHT%% ==fine-tuning approach, such asthe Generative Pre-trained Transformer (OpenAIGPT)== %%POSTFIX%%(Radford et al., 2018), introdu*
>%%LINK%%[[#^zt3jrvvdsfa|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zt3jrvvdsfa


>%%
>```annotation-json
>{"created":"2022-03-09T09:48:41.499Z","updated":"2022-03-09T09:48:41.499Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":2785,"end":2941},{"type":"TextQuoteSelector","exact":"The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training.","prefix":"for the fine-tuning approaches. ","suffix":" Forexample, in OpenAI GPT, the "}]}]}
>```
>%%
>*%%PREFIX%%for the fine-tuning approaches.%%HIGHLIGHT%% ==The ma-jor limitation is that standard language models areunidirectional, and this limits the choice of archi-tectures that can be used during pre-training.== %%POSTFIX%%Forexample, in OpenAI GPT, the*
>%%LINK%%[[#^o2hwd6xbuqm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^o2hwd6xbuqm


>%%
>```annotation-json
>{"created":"2022-03-09T09:53:43.993Z","updated":"2022-03-09T09:53:43.993Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":3400,"end":3514},{"type":"TextQuoteSelector","exact":"improve the fine-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representations from Transformers.","prefix":"th directions.In this paper, we ","suffix":"BERT alleviates the previously m"}]}]}
>```
>%%
>*%%PREFIX%%th directions.In this paper, we%%HIGHLIGHT%% ==improve the fine-tuning basedapproaches by proposing BERT: BidirectionalEncoder Representations from Transformers.== %%POSTFIX%%BERT alleviates the previously m*
>%%LINK%%[[#^gmcdfzan7dl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gmcdfzan7dl


>%%
>```annotation-json
>{"created":"2022-03-09T10:01:07.238Z","updated":"2022-03-09T10:01:07.238Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":3519,"end":3649},{"type":"TextQuoteSelector","exact":"alleviates the previously mentioned unidi-rectionality constraint by using a â€œmasked lan-guage modelâ€ (MLM) pre-training objective","prefix":"ntations from Transformers.BERT ","suffix":", in-spired by the Cloze task (T"}]}]}
>```
>%%
>*%%PREFIX%%ntations from Transformers.BERT%%HIGHLIGHT%% ==alleviates the previously mentioned unidi-rectionality constraint by using a â€œmasked lan-guage modelâ€ (MLM) pre-training objective== %%POSTFIX%%, in-spired by the Cloze task (T*
>%%LINK%%[[#^uv27oax3zr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^uv27oax3zr


>%%
>```annotation-json
>{"created":"2022-03-09T10:27:48.204Z","updated":"2022-03-09T10:27:48.204Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":4123,"end":4248},{"type":"TextQuoteSelector","exact":"to the masked language model, we also usea â€œnext sentence predictionâ€ task that jointly pre-trains text-pair representations.","prefix":"ional Transformer. In addi-tion ","suffix":" The contributionsof our paper a"}]}]}
>```
>%%
>*%%PREFIX%%ional Transformer. In addi-tion%%HIGHLIGHT%% ==to the masked language model, we also usea â€œnext sentence predictionâ€ task that jointly pre-trains text-pair representations.== %%POSTFIX%%The contributionsof our paper a*
>%%LINK%%[[#^b5kk5x8xsz|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^b5kk5x8xsz


>%%
>```annotation-json
>{"created":"2022-03-09T10:35:15.645Z","text":"2 Related Work","updated":"2022-03-09T10:35:15.645Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":5178,"end":5192},{"type":"TextQuoteSelector","exact":"2 Related Work","prefix":"github.com/google-research/bert.","suffix":"There is a long history of pre-t"}]}]}
>```
>%%
>*%%PREFIX%%github.com/google-research/bert.%%HIGHLIGHT%% ==2 Related Work== %%POSTFIX%%There is a long history of pre-t*
>%%LINK%%[[#^246loi456bu|show annotation]]
>%%COMMENT%%
>2 Related Work
>%%TAGS%%
>
^246loi456bu


>%%
>```annotation-json
>{"created":"2022-03-09T11:13:00.075Z","text":"3 BERT","updated":"2022-03-09T11:13:00.075Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":9784,"end":9790},{"type":"TextQuoteSelector","exact":"3 BERT","prefix":"., 2009; Yosinski et al., 2014).","suffix":"We introduce BERT and its detail"}]}]}
>```
>%%
>*%%PREFIX%%., 2009; Yosinski et al., 2014).%%HIGHLIGHT%% ==3 BERT== %%POSTFIX%%We introduce BERT and its detail*
>%%LINK%%[[#^bdlxr4lwsym|show annotation]]
>%%COMMENT%%
>3 BERT
>%%TAGS%%
>
^bdlxr4lwsym


>%%
>```annotation-json
>{"created":"2022-03-09T11:30:40.992Z","updated":"2022-03-09T11:30:40.992Z","document":{"title":"BERT.pdf","link":[{"href":"urn:x-pdf:dccb9bc542f22b2bdd94110918c68f96"},{"href":"vault:/notes/AI/files/BERT.pdf"}],"documentFingerprint":"dccb9bc542f22b2bdd94110918c68f96"},"uri":"vault:/notes/AI/files/BERT.pdf","target":[{"source":"vault:/notes/AI/files/BERT.pdf","selector":[{"type":"TextPositionSelector","start":9925,"end":10019},{"type":"TextQuoteSelector","exact":"Dur-ing pre-training, the model is trained on unlabeleddata over different pre-training tasks.","prefix":": pre-training and fine-tuning. ","suffix":" For fine-tuning, the BERT model"}]}]}
>```
>%%
>*%%PREFIX%%: pre-training and fine-tuning.%%HIGHLIGHT%% ==Dur-ing pre-training, the model is trained on unlabeleddata over different pre-training tasks.== %%POSTFIX%%For fine-tuning, the BERT model*
>%%LINK%%[[#^tez5056t4a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tez5056t4a
