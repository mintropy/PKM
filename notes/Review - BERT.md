---
type : knowledges
detail : 
content_type :
---

[[AI]]
created : 2022-03-09 16:50
tags : #ğŸ–¥ï¸/AI/Review  

# Review - BERT
- Pre-training of Deep Bidirectional Transformers for Language Understanding

---
# Note

---
## [[paper - BERT#^2u26epoo7bn|Abstract]]
- pre-train â†’ fine-tuned ìœ¼ë¡œ SOTA ë‹¬ì„±

---
## [[paper - BERT#^gom72aonwvl|1 Introduction]]
- pre-trained ì˜ í‘œí˜„ ë°©ì‹ìœ¼ë¡œ feature-based, fine-tuning ì´ ìˆìŒ
- feature-based approach
	- ELMo
- fine-tuning approach
	- Generatice Pre-trained Transformer (OpenAI GPT)
- ê¸°ì¡´ ì–¸ì–´ ëª¨ë¸ì€ ë‹¨ë°©í–¥(unidirectional)
	- OpenAI GPTì˜ ê²½ìš° ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë³´ëŠ” ë°©ì‹
	- ëª¨ë“  í† í°ì´ Transformerì˜  self-attentionì—ì„œ ì´ì „ í† í°ì—ë§Œ ì°¸ì—¬
- BERTë¥¼ ì ìš© (Transformerì˜ ì–‘ë°©í–¥ Encoder í‘œí˜„)
	- ë‹¨ë°©í–¥ê³¼ ê´€ë ¨í•œ ë¬¸ì œë¥¼ MLM(masked language model) pre-training ìœ¼ë¡œ ì ‘ê·¼
	- ì„ì˜ë¡œ ì…ë ¥ì˜ í† í°ì„ì„ maskí•˜ì—¬ ì˜ˆì¸¡
	- ê¸°ì¡´ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë°”ë¼ë³´ëŠ” pre-trainging ì–¸ì–´ ëª¨ë¸ê³¼ ë‹¬ë¦¬, MLMì€ ì–‘ë°©í–¥ ì°¸ì¡°ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨
	- ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ ì‘ì—…ì„ í•¨

---
## [[paper - BERT#^246loi456bu|2 Related Work]]
### 2.1 Unsupervised Feature-based Approaches

### 2.2 Unsupervised Fine-tuning Approaches

### 2.3 Transfer Learning from Supervised Data

---
## [[paper - BERT#^bdlxr4lwsym|3 BERT]]
- fine-tuning

---
# ì°¸ì¡° 
